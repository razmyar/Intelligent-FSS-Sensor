{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports  & Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import imutils\n",
    "import seaborn as sns\n",
    "import platform\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import keras\n",
    "from keras import backend as K, optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "rcParams['font.family'] = 'serif'\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "np.random.seed(44)\n",
    "train_step_size = 0\n",
    "valid_step_size = 0\n",
    "test_step_size = 0\n",
    "y_oneHot = []\n",
    "y_normalized = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataConfig(object):\n",
    "    if platform.system()[2] == 'xenial':\n",
    "        print(\"[INFO] Running at home\")\n",
    "        ROOT = \"/home/Develope/dataset/\"\n",
    "    elif platform.system() == 'Linux':\n",
    "        print(\"[INFO] Running on Linux\")\n",
    "        ROOT = \"/home/Develope/dataset/\"  # in Linux\n",
    "    else:\n",
    "        print(\"[INFO] Running on Mac\")\n",
    "        ROOT = \"/Users/../Develope/dataset/\"  # in Mac\n",
    "\n",
    "    DATA_PATH = ROOT + \"experiments/\"\n",
    "    SET_DIR = ROOT + \"sets/CNN/\"\n",
    "    SAVE_DIR = ROOT + \"weights/CNN/\"\n",
    "    DATA_SET = ROOT + \"@List/data_DL.csv\"  # dataset_noSmooth  #Dataset.csv\n",
    "    IMG_EXTENTION = \".tiff\"\n",
    "    IMG_SAVE_DIR = ROOT + \"charts/\"\n",
    "\n",
    "    TRAIN_TEST_RATIO = 0.80\n",
    "    TRAIN_VAL_RATIO = 0.80\n",
    "\n",
    "    GEN_SETS = False\n",
    "\n",
    "    # ORG IMG=272\n",
    "    IMG_RESHAPE = True\n",
    "    IMG_HEIGHT = 300\n",
    "    IMG_WIDTH = IMG_HEIGHT\n",
    "    IMG_CHANNEL_NUM = 1\n",
    "\n",
    "    EPOCH_NUM = 30\n",
    "    BATCH_SIZE = 25  # 30\n",
    "    WORKERS_NUM = 2  # 2\n",
    "\n",
    "\n",
    "config = DataConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load / Generate sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(path, name=\"\"):\n",
    "    print(\"[INFO] Loading \" + name + \".\")\n",
    "    df_list = pd.read_csv(path, converters={'img_path': lambda x: str(x)})\n",
    "    df_list.index.name = \"index\"\n",
    "    print(\"[INFO] Done. \")\n",
    "    return df_list\n",
    "\n",
    "\n",
    "dataset_org = load_dataset(config.DATA_SET, name=\"data_DL\")\n",
    "data = dataset_org.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.unique(data[['Label']])\n",
    "label_transformer = LabelBinarizer()\n",
    "label_transformer.classes_ = labels\n",
    "label_transformer.y_type_ = \"multiclass\"\n",
    "label_transformer.sparse_input_ = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_val_test(dataFrame):\n",
    "    shuffled_indices = np.random.permutation(len(dataFrame))\n",
    "\n",
    "    train_size = int(len(dataFrame) * config.TRAIN_TEST_RATIO)\n",
    "    valid_size = int(train_size * config.TRAIN_VAL_RATIO)\n",
    "\n",
    "    train_indices = shuffled_indices[:train_size]\n",
    "    test_indices = shuffled_indices[train_size:]\n",
    "\n",
    "    val_indices = train_indices[valid_size:]\n",
    "    train_indices = train_indices[:valid_size]\n",
    "\n",
    "    train_data = dataFrame.iloc[train_indices]\n",
    "    val_data = dataFrame.iloc[val_indices]\n",
    "    test_data = dataFrame.iloc[test_indices]\n",
    "\n",
    "    global train_step_size, valid_step_size, test_step_size\n",
    "\n",
    "    train_step_size = len(train_data)\n",
    "    train_step_size = train_step_size - train_step_size % config.BATCH_SIZE\n",
    "    print(\"Training Steps: \", train_step_size)\n",
    "\n",
    "    valid_step_size = len(val_data)\n",
    "    valid_step_size = valid_step_size - valid_step_size % config.BATCH_SIZE\n",
    "    print(\"Validation Steps: \", valid_step_size)\n",
    "\n",
    "    test_step_size = len(test_data)\n",
    "    test_step_size = test_step_size - test_step_size % config.BATCH_SIZE\n",
    "    print(\"Test Steps: \", test_step_size)\n",
    "\n",
    "    train_data.to_csv(config.SET_DIR + \"train_Direction.csv\", index=False)\n",
    "    print(\"\\nTrain set:\", config.SET_DIR + \"train_Direction.csv\")\n",
    "\n",
    "    val_data.to_csv(config.SET_DIR + \"val_Direction.csv\", index=False)\n",
    "    print(\"Valid set:\", config.SET_DIR + \"val_Direction.csv\")\n",
    "\n",
    "    test_data.to_csv(config.SET_DIR + \"test_Direction.csv\", index=False)\n",
    "    print(\"Test  set:\", config.SET_DIR + \"test_Direction.csv\")\n",
    "\n",
    "    np.savetxt(config.SET_DIR + 'sets_Direction.txt',\n",
    "               (train_step_size, valid_step_size, test_step_size),\n",
    "               fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.GEN_SETS:\n",
    "    generate_train_val_test(data)\n",
    "\n",
    "else:\n",
    "    print(\"[INFO] Loading sets.........\\n\")\n",
    "    sets = np.loadtxt(config.SET_DIR + 'sets_Direction.txt')\n",
    "    train_step_size, valid_step_size, test_step_size = int(sets[0]), int(sets[1]), int(sets[2])\n",
    "\n",
    "    print(\"[INFO] Training Steps:\" + \"{:12d}\".format(train_step_size))\n",
    "    print(\"[INFO] Validation Steps:\" + \"{:10d}\".format(valid_step_size))\n",
    "    print(\"[INFO] Test Steps:\" + \"{:16d}\".format(test_step_size))\n",
    "\n",
    "path_train = config.SET_DIR + \"train_Direction.csv\"\n",
    "path_val = config.SET_DIR + \"val_Direction.csv\"\n",
    "path_test = config.SET_DIR + \"test_Direction.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "\n",
    "def normalize(img, min_range=0.0, max_range=1.0):\n",
    "    return min_range + (img * (max_range - min_range)) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, path, config, lbl_transformer=label_transformer, shuffle=True, display=False,\n",
    "                 return_original=False):\n",
    "\n",
    "        # initialization'\n",
    "        self.df_train = None\n",
    "        self.num_iter = 0\n",
    "        self.indexes = 0\n",
    "        self.dim = (config.IMG_HEIGHT, config.IMG_WIDTH)\n",
    "        self.shuffle = shuffle\n",
    "        self.display = display\n",
    "        self.return_y = return_original\n",
    "        self.lbl_transformer = lbl_transformer\n",
    "        self.read_dataFrame(path)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def read_dataFrame(self, path):\n",
    "        self.df_train = pd.read_csv(path, header=0, converters={'img_path': lambda x: str(x)})\n",
    "\n",
    "        self.df_train = self.df_train.sample(frac=1).reset_index(drop=True)\n",
    "        self.num_iter = len(self.df_train) // config.BATCH_SIZE\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        step = len(self.df_train) // config.BATCH_SIZE\n",
    "        return step\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.df_train))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * config.BATCH_SIZE:(index + 1) * config.BATCH_SIZE]\n",
    "\n",
    "        # find rows\n",
    "        list_rows = [self.df_train.iloc[k] for k in indexes]\n",
    "        X, y = self.__data_generation(list_rows)\n",
    "\n",
    "        if self.return_y:\n",
    "            global y_oneHot\n",
    "            y_oneHot.append(y)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def __data_generation(self, list_rows):\n",
    "\n",
    "        X = np.empty((config.BATCH_SIZE, config.IMG_HEIGHT, config.IMG_WIDTH, config.IMG_CHANNEL_NUM))\n",
    "        y = np.empty((config.BATCH_SIZE, 1), np.float32)\n",
    "\n",
    "        for i, row in enumerate(list_rows):\n",
    "\n",
    "            img_path = row[0]\n",
    "            if platform.system() == 'Linux':\n",
    "                img_path = config.ROOT + img_path[41:]\n",
    "            img = cv2.imread(img_path)\n",
    "            if config.IMG_RESHAPE:\n",
    "                img = imutils.resize(img, width=config.IMG_WIDTH)\n",
    "            img = grayscale(img)\n",
    "            img = normalize(img)\n",
    "\n",
    "            img = img.reshape(config.IMG_HEIGHT, config.IMG_WIDTH, config.IMG_CHANNEL_NUM)\n",
    "\n",
    "            X[i,] = img\n",
    "            y[i] = (row[1]).astype(np.float16).reshape([-1])\n",
    "\n",
    "            if self.display:\n",
    "                print(\"Label: \", (row[1]).astype(np.float16).reshape([-1]))\n",
    "\n",
    "            if self.return_y:\n",
    "                global y_normalized\n",
    "                y_normalized.append((row[2]).astype(np.float16).reshape([-1]))\n",
    "\n",
    "        y = self.lbl_transformer.transform(np.array(y))\n",
    "        # print(y)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_normalized=[]\n",
    "# y_oneHot=[]\n",
    "# test = DataGenerator(path_train,config,display=False,return_original=True,shuffle=True)\n",
    "# x = test[1]\n",
    "# x[1][2]\n",
    "# y_oneHot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def conv_block(self, model_name):\n",
    "        row = config.IMG_HEIGHT\n",
    "        col = config.IMG_WIDTH\n",
    "        ch = config.IMG_CHANNEL_NUM\n",
    "\n",
    "        model = Sequential(name=model_name)\n",
    "        K.set_image_data_format('channels_last')\n",
    "        model.add(Conv2D(36,  # 32,\n",
    "                         kernel_size=(5, 5),\n",
    "                         strides=(2, 2),\n",
    "                         padding=\"same\",\n",
    "                         activation='relu',\n",
    "                         name=\"conv2d_1\",\n",
    "                         input_shape=(row, col, ch)))\n",
    "\n",
    "        model.add(Conv2D(36,  # 32,\n",
    "                         kernel_size=(5, 5),\n",
    "                         strides=(2, 2),\n",
    "                         padding=\"same\",\n",
    "                         activation='relu',\n",
    "                         name=\"conv2d_2\"\n",
    "                         ))\n",
    "\n",
    "        model.add(Conv2D(48,  # 32,\n",
    "                         kernel_size=(5, 5),\n",
    "                         strides=(2, 2),\n",
    "                         padding=\"same\",\n",
    "                         activation='relu',\n",
    "                         name=\"conv2d_3\"\n",
    "                         ))\n",
    "\n",
    "        model.add(Conv2D(64,  # 32,\n",
    "                         kernel_size=(3, 3),\n",
    "                         strides=(2, 2),\n",
    "                         padding=\"same\",\n",
    "                         activation='relu',\n",
    "                         name=\"conv2d_4\"\n",
    "                         ))\n",
    "\n",
    "        model.add(Conv2D(64,  # 32,\n",
    "                         kernel_size=(3, 3),\n",
    "                         strides=(2, 2),\n",
    "                         padding=\"same\",\n",
    "                         activation='relu',\n",
    "                         name=\"conv2d_5\"\n",
    "                         ))\n",
    "\n",
    "        model.add(Flatten(name=\"flatten_1\"))\n",
    "        return model\n",
    "\n",
    "    def CNN_direction(self):\n",
    "        model = self.conv_block(model_name=\"CNN_Pred\")\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(100))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(50))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(20))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(25, activation='softmax'))\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                      optimizer=keras.optimizers.Adadelta(),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        print('CNN is created and compiled..')\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def CNN_reg(self):\n",
    "        model = self.conv_block(model_name=\"CNN_reg\")\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(100))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(50))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(20))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        adam_custom = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "        model.compile(optimizer=adam_custom, loss=\"mse\")\n",
    "        print('CNN_reg is created and compiled..')\n",
    "        print(model.summary())\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().CNN_direction()\n",
    "model_save_path = config.SAVE_DIR + str(model.name) + \"-{epoch:02d}-{val_accurasy:.2f}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "checkpoint = ModelCheckpoint(model_save_path, monitor='val_acc', verbose=0, save_best_only=True)\n",
    "\n",
    "# TensorBoard\n",
    "vis = TensorBoard(log_dir=config.ROOT + \"log/CNN/\",\n",
    "                  batch_size=config.BATCH_SIZE,\n",
    "                  write_graph=False,\n",
    "                  write_images=False)\n",
    "\n",
    "callback_list = [TQDMNotebookCallback(leave_inner=False, leave_outer=True), checkpoint, vis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Validation / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(path_train, config)\n",
    "valid_generator = DataGenerator(path_val, config)\n",
    "test_generator = DataGenerator(path_test, config, return_original=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model.fit_generator(generator=train_generator,\n",
    "#                     validation_data=valid_generator,\n",
    "#                     verbose=0,\n",
    "#                     callbacks=callback_list,\n",
    "#                     epochs=config.EPOCH_NUM,\n",
    "#                     use_multiprocessing=True,\n",
    "#                     workers=config.WORKERS_NUM\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_oneHot = []\n",
    "y_normalized = []\n",
    "\n",
    "model.load_weights(config.SAVE_DIR + \"trained_model.hdf5\")\n",
    "predict = model.predict_generator(generator=test_generator,\n",
    "                                  verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Conf. Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_oneHot_arr = np.asarray(y_oneHot)\n",
    "y_oneHot_arr = y_oneHot_arr.reshape(y_oneHot_arr.shape[0] * y_oneHot_arr.shape[1], y_oneHot_arr.shape[2])\n",
    "y_test = label_transformer.inverse_transform(predict)\n",
    "y_hat = label_transformer.inverse_transform(y_oneHot_arr)\n",
    "y_hat = y_hat[:predict.shape[0]]\n",
    "y_normalized = y_normalized[:19575]\n",
    "y_normalized_arr = np.asarray(y_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.roll(labels, 1)\n",
    "matrix = confusion_matrix(y_test, y_hat)\n",
    "matrix = matrix / matrix.astype(np.float).sum(axis=1)\n",
    "df_cm = pd.DataFrame(matrix,\n",
    "                     index=[i for i in labels],\n",
    "                     columns=[i for i in labels]).round(2)\n",
    "\n",
    "axis_labels = np.array(['center', 0, 15, 30, 45, 60, 75, 90,\n",
    "                        105, 120, 135, 150, 165, 180, 195, 210, 225,\n",
    "                        240, 255, 270, 285, 300, 315, 330, 345], dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig_size = 5.05\n",
    "font_size = 8\n",
    "plt.figure(figsize=(fig_size, fig_size))\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "plot = sns.heatmap(df_cm,\n",
    "                   annot=True,\n",
    "                   square=0,\n",
    "                   xticklabels=axis_labels,\n",
    "                   yticklabels=axis_labels,\n",
    "                   annot_kws={\"size\": font_size, 'rotation': 0},\n",
    "                   fmt='.0%',  # add %\n",
    "                   linecolor='black',\n",
    "                   linewidths=0.5,\n",
    "                   cbar=0,\n",
    "                   cmap='gist_yarg'  # \"YlGn\" #viridis\n",
    "                   )  # font size\n",
    "\n",
    "for txt in plot.texts:\n",
    "    if txt.get_text() == '0%':\n",
    "        txt.set_text('')\n",
    "    else:\n",
    "        txt.set_text(txt.get_text().replace('%', ''))\n",
    "\n",
    "    if txt.get_text() == '100':\n",
    "        txt.set_size(7)\n",
    "\n",
    "plot.xaxis.set_ticks_position('top')\n",
    "plot.yaxis.set_ticks_position('left')\n",
    "plt.xticks(size=font_size, rotation=90)\n",
    "plt.yticks(size=font_size, rotation=0)\n",
    "\n",
    "axes = plot.axes\n",
    "axes.set_ylim(len(df_cm) + 0.01, -0.01)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(config.IMG_SAVE_DIR+\"razmy5\",dpi=350, bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Location of the mis-classifed samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame()\n",
    "df_results['y_normalized'] = y_normalized_arr.reshape(19575)\n",
    "df_results['y'] = y_test\n",
    "df_results['y_hat'] = y_hat\n",
    "\n",
    "df_results = df_results[['y_hat', 'y']].assign(is_correct = df_results.y_hat.astype(str) == df_results.y.astype(str))\n",
    "df_results['y_normalized'] = y_normalized_arr.reshape(19575)\n",
    "\n",
    "df_error = df_results.loc[df_results['is_correct'] == False]\n",
    "df_error = df_error.sort_values(by=['y'])\n",
    "df_error = df_error.loc[df_error['y'] >= 0]\n",
    "\n",
    "df_correct = df_results.loc[df_results['is_correct'] == True]\n",
    "df_correct = df_correct.sort_values(by=['y'])\n",
    "df_correct = df_correct.loc[df_correct['y'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_style('white', {'axes.linewidth': 0.5})\n",
    "plt.rcParams['xtick.major.width'] = 1\n",
    "plt.rcParams['xtick.bottom'] = True\n",
    "plt.rcParams['ytick.left'] = True\n",
    "factor=20\n",
    "\n",
    "font_size=18\n",
    "plt.figure(figsize = (10,5))\n",
    "\n",
    "x_values = df_error['y_normalized']\n",
    "x_values = x_values-np.mean(x_values)\n",
    "x_values*=factor/2\n",
    "\n",
    "sns.distplot(x_values  , color='r',   hist=0,\n",
    "                \n",
    "            kde_kws={\"color\": \"r\", \"lw\": 3.5, 'ls':'--'},\n",
    "             label=\"miss-classified\\n\"+\n",
    "             \"\\u03BC:\" + str(round(np.mean(x_values),3))+ \n",
    "             \"   \\u03B4:\"+ str(round(np.std(x_values),2)))\n",
    "\n",
    "y = df_correct['y_normalized']\n",
    "\n",
    "y_scaled = y-np.mean(y)\n",
    "y_scaled*=factor\n",
    "ax=sns.distplot(y_scaled  , color='k', hist=0, bins=100,\n",
    "                kde_kws={\"color\": \"k\", \"lw\": 3.5},\n",
    "            label=\"\\ncorrectly-classified\\n\"+\n",
    "             \"\\u03BC:\" + str(round(np.mean(y_scaled),3))+ \n",
    "             \"   \\u03B4:\"+ str(round(np.std(y_scaled),2))\n",
    "            )\n",
    "\n",
    "ax.xaxis.grid(color='gray', linestyle='--', linewidth=0.05)\n",
    "ax.yaxis.grid(color='gray', linestyle='--', linewidth=0.05)\n",
    "\n",
    "\n",
    "plt.legend(fontsize=font_size)\n",
    "plt.xlabel('Tip deflection from the resting position (deg)', size=font_size+2) \n",
    "plt.ylabel('PDF', size=font_size)\n",
    "plt.axvline(0, color='b',linestyle='-')\n",
    "plt.xticks(size=font_size)\n",
    "plt.yticks(size=font_size)\n",
    "\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize=font_size) # for legend text\n",
    "plt.savefig(config.IMG_SAVE_DIR+\"deviation\",dpi=350, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test_generator[0]\n",
    "img = sample[0][2].reshape(300,300)\n",
    "plt.imshow(img, cmap='hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_activation(activations, col_size, row_size, act_index):\n",
    "    activation = activations[act_index]\n",
    "    print(\"layer: \", str(act_index + 1), activation.shape)\n",
    "    activation_index = 0\n",
    "\n",
    "    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size * 3, col_size * 3))\n",
    "\n",
    "    for row in range(0, row_size):\n",
    "        for col in range(0, col_size):\n",
    "            ax[row][col].imshow(activation[0, :, :, activation_index], cmap='hot')\n",
    "            activation_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "activations = activation_model.predict(img.reshape(1, 300, 300, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_activation(activations, 6, 6, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
